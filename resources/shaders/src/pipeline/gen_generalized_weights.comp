#version 460 core

// Enable necessary subgroup extensions
#extension GL_KHR_shader_subgroup_basic      : require
#extension GL_KHR_shader_subgroup_arithmetic : require
#extension GL_KHR_shader_subgroup_clustered  : require

#pragma optionNV(fastmath on)
#pragma optionNV(fastprecision on)

#include <guard.glsl>
#include <math.glsl>
#include <bary.glsl>

// General layout rule declarations
layout(local_size_x = 256) in;
layout(std430)             buffer;
layout(std140)             uniform;

// Shader storage buffer declarations
layout(binding = 0) restrict readonly  buffer b_vert { vec3  data[]; } vert_in;
layout(binding = 1) restrict readonly  buffer b_elem { uvec3 data[]; } elem_in;
layout(binding = 2) restrict readonly  buffer b_colr { vec3  data[]; } colr_in;
layout(binding = 3) restrict writeonly buffer b_bary { Bary4 data[]; } bary_out;

// Uniform buffer declarations
layout(binding = 0) uniform b_unif {
  uint n;
  uint n_verts;
  uint n_elems;
} unif;

const uint sg_cluster_size = 2; // 2 units per set of barycentric weights
      uint sg_cluster_offs = gl_SubgroupInvocationID.x % sg_cluster_size;

// Shared memory declarations
shared vec3  s_verts_in[generalized_weights];
shared uvec3 s_elems_in[generalized_weights * 4];
shared Bary4 s_bary_out[gl_WorkGroupSize.x / sg_cluster_size];
       uint  s_bary_i = gl_LocalInvocationID.x / sg_cluster_size;

void load_shared_data() {
  for (uint i = gl_LocalInvocationID.x; i < unif.n_verts; i += gl_WorkGroupSize.x)
    s_verts_in[i] = vert_in.data[i];
  for (uint i = gl_LocalInvocationID.x; i < unif.n_elems; i += gl_WorkGroupSize.x)
    s_elems_in[i] = elem_in.data[i];

  memoryBarrierShared();
  barrier();
}

float norm_weights() {
  vec4 n = vec4(0);
  for (uint i = sg_cluster_offs; i < generalized_weights_4; i += sg_cluster_size)
    n += s_bary_out[s_bary_i][i];
  return hsum(subgroupClusteredAdd(n, sg_cluster_size));
}

void normalize_weights() {
  float n = 1.f / norm_weights();
  for (uint i = sg_cluster_offs; i < generalized_weights_4; i += sg_cluster_size)
    s_bary_out[s_bary_i][i] *= n;

  subgroupMemoryBarrierShared();
  barrier();
}

void clear_weights(float value) {
  for (uint i = sg_cluster_offs; i < generalized_weights_4; i += sg_cluster_size)
    s_bary_out[s_bary_i][i] = vec4(value);

  subgroupMemoryBarrierShared();
  barrier();
}

void scatter_weights(in vec3 w_, in uvec3 i) {
  const uvec3 i_ = i / 4;
  const uvec3 j_ = i % 4;

  for (uint k = 0; k < sg_cluster_size; ++k) {
    if (k == sg_cluster_offs) {
      s_bary_out[s_bary_i][i_[0]][j_[0]] += w_[0];
      s_bary_out[s_bary_i][i_[1]][j_[1]] += w_[1];
      s_bary_out[s_bary_i][i_[2]][j_[2]] += w_[2];
    }

    subgroupMemoryBarrierShared();
    barrier();
  }
}

// Enum flags for per-triangle MVC computation return type
#define MVCReturnOnVertex   0x01u
#define MVCReturnOnTriangle 0x02u
#define MVCReturnOnPlane    0x04u
#define MVCReturnCorrect    0x08u

// Return result for per-triangle MVC computation
struct MVCObject {
  vec3 weight;
  uint type;
};

MVCObject mean_value_coords_per_tri(in vec3 p, in uvec3 el) {
  // Compute per-vertex direction vectors and vector lengths for this particular element
  mat3 u = mat3((s_verts_in[el[0]] - p), (s_verts_in[el[1]] - p), (s_verts_in[el[2]] - p));
  vec3 d = vec3(length(u[0]), length(u[1]), length(u[2]));

  // Fallbacks: test if x lies near or on a vertex, weight that vertex as 1 and return directly
  bvec3 on_vert_mask = lessThan(d, vec3(M_EPS));
  if (any(on_vert_mask))
    return MVCObject(mix(vec3(0), vec3(1), on_vert_mask), MVCReturnOnVertex);
  
  // Normalize vector directions  in 'u'
  for (uint i = 0; i < 3; ++i)
    u[i] /= d[i];
  
  // Compute spherical triangle data
  vec3 theta = 2.f * asin(.5f * vec3(length(u[1] - u[2]), length(u[2] - u[0]), length(u[0] - u[1])));
  vec3 sin_theta = sin(theta);
  float h = .5f * hsum(theta);
    
  // Fallback: test if x lies near or on triangle, return 2D barycentric coordinates
  if (abs(M_PI - h) <= M_EPS)
    return MVCObject(sin_theta * d.yzx * d.zxy, MVCReturnOnTriangle);

  vec3 c = (2.f * sin(h) * sin(h - theta)) / (sin_theta.yzx * sin_theta.zxy) - 1.f;
  vec3 s = sign(determinant(u)) * sqrt(max(1.f - c * c, M_EPS));

  // Fallback: test if x lies on triangle's plane but way outside triangle, set weights to 0
  if (any(lessThanEqual(abs(s), vec3(M_EPS))))
    return MVCObject(vec3(0), MVCReturnOnPlane);

  // Finally; compute actual weight and return
  vec3 w = (theta - c.yzx * theta.zxy - c.zxy * theta.yzx) / (d * sin_theta.yzx * s.zxy);
  return MVCObject(w, MVCReturnCorrect);
}

void mean_value_coords_total(in vec3 p) {
  clear_weights(0.f);

  uint j_over = unif.n_elems % sg_cluster_size;
  uint j_maxm = unif.n_elems + j_over;

  for (uint j = sg_cluster_offs; j < j_maxm; j += sg_cluster_size) {
    uvec3 el = uvec3(0);
    
    MVCObject result_self = MVCObject(vec3(0), MVCReturnCorrect);
    if (j < j_maxm - j_over) {
      el = s_elems_in[j];
      result_self = mean_value_coords_per_tri(p, el);
    };

    // Test for special cases which return their own value directly
    bool clear_self = result_self.type == MVCReturnOnVertex || result_self.type == MVCReturnOnTriangle;
    bool clear_return = subgroupClusteredOr(clear_self, sg_cluster_size);
    
    // Kill invalid results
    if ((clear_return && !clear_self) || any(isnan(result_self.weight)))
      result_self.weight = vec3(0);
    
    // Given special cases which return their own value directly, set all weights to zero
    if (clear_return)
      clear_weights(0.f);

    scatter_weights(result_self.weight, el);

    // Given special cases which return their own value directly, exit early
    if (clear_return)
      break;
  }

  normalize_weights();
}

void main() {
  load_shared_data();
  
  const uint i = gl_GlobalInvocationID.x / sg_cluster_size;
  guard(i < unif.n);

  // Load position data across subgroup cluster
  vec3 p = subgroupClusteredAdd(sg_cluster_offs == 0 ? colr_in.data[i] : vec3(0), sg_cluster_size);

  // Main computational body
  mean_value_coords_total(p);

  // Share result store across subgroup cluster
  for (uint k = sg_cluster_offs; k < generalized_weights_4; k += sg_cluster_size)
    bary_out.data[i][k] = s_bary_out[s_bary_i][k];
}